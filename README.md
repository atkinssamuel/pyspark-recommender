# PySpark Recommender System

## Conda Environment Setup
**Create environment from environment.yml**:  
*From base directory:*  
```conda env create -f ./environment.yml```

**Update environment from environment.yml**:  
*From base directory and after activating existing environment:*  
```conda env update --file ./environment.yml```


## Spark Explained
Apache Spark is an in-memory computing framework that provides faster processing capability. It is commonly used in ML and other data processing tasks. The combination of Python and the Spark API is called PySpark. To install PySpark using conda, simply add "pyspark" to your project-specific .yml file and update your environment.

After running your first PySpark application, execution information about the tasks you are running is available at <http://localhost:4040/jobs/>. 

## PySpark Application Exercises


