# PySpark Recommender System

## Conda Environment Setup
**Create environment from environment.yml**:  
*From base directory:*  
```conda env create -f ./environment.yml```

**Update environment from environment.yml**:  
*From base directory and after activating existing environment:*  
```conda env update --file ./environment.yml```


## Spark Explained
Apache Spark is an in-memory computing framework that provides faster processing capability. It is commonly used in ML and other data processing tasks. 

